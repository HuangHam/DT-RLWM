import numpy as np  # for numeric calculations
import math
import time
import func as f
import pandas as pd
data, te_data = f.stimSeq(pd.read_csv("../Subject_Data/DT-RLWM-version4.csv")), f.stimSeq(pd.read_csv("../Subject_Data/DT-RLWM-version4-te.csv"))
# RLWM model agents
"""
take policy at the end of learning, take the log likelihood of RL policy. In testing, we have actual choices. Tells you probability of choosing the choice of actual choice. 
"""
class DT_RLWM_Models:
    '''
    The generic superclass for all DT-RLWM Models. It takes in the dataset and use it to guide the model simulation and model fitting.
    '''
    def __init__(self, Data = data, te_Data = te_data):
        self.colnamesIn = ['block', 'dt', 'ns', 'numTrial', 'corAct', 'action', 'stimSeq'] #input-column names need to guide agent simulating
        self.colnamesOut = ['subject', 'block', 'dt', 'ns', 'numTrial', 'action', 'Cor', 'corAct', 'stimSeq', 'simulation'] #output-columns generated by simulating agent
        self.colnamesIn_te = ['block', 'dt', 'ns', 'corAct', 'stimSeq']  # output-columns generated by simulating agent
        self.colnamesOut_te = ['subject', 'block', 'dt', 'ns', 'action', 'Cor','stimSeq']  # output-columns generated by simulating agent
        # sort the data by subject and block
        self.data = Data.sort_values(by=['subject','block'])
        self.te_data =  te_Data.sort_values(by=['subject','block'])
        # carve the data into individual subject data and select the input columns. Put them into a list.
        self.Sub = np.unique(self.data['subject'])  # list of subject numbers
        self.Block = np.unique(self.data['block'])
        self.subject_data = [f.pd2np(self.data[self.data.subject == n][self.colnamesIn]) for n in self.Sub]
        self.subject_data_te = [f.pd2np(te_data[te_data.subject == n][self.colnamesIn_te]) for n in self.Sub]
        # Certain model parameters
        self.Krange = (2,3,4) #the range of WM capacity we want to test
        self.KapDiscount = (1,) #the discounted WM capacity in DT condition
        self.te_pname = ['retentionST', 'retentionDT']
        self.te_numParams = len(self.te_pname)
        self.retention = 1
        #parse subject data into a list, select only relevant columns, and convert them into numpy
    '''
    learning Data Simulation and Fitting
    '''
    def Sim_agent(self, nSim=100):
        '''
        :param Param: The simulating parameters of all subjects (list of lists)
        :param Kap: WM capacity of all subjects (int list)
        :param nSim: number of simulation per subject
        :return: a pandas dataframe
        '''
        print('now simulating '+self.name)
        ModFit = np.load('../ModelFit/ModFit' + self.name + '.npz', allow_pickle=True)  # loads your saved array into variable a.
        Param, Kap = ModFit['Params'], ModFit['Ks']
        Data = np.ones(len(self.colnamesOut))  # initialize output learning data
        #EndPolicy = np.empty((len(self.Sub), nSim, len(self.Block)), dtype = list)
        for n in self.Sub:#for every subject
            print('subject ' + str(n))
            p, k = Param[n - 1], Kap[n - 1] #get the subject's parameter values
            for s in range(nSim): #for every simulation
                sub, endPolicy = self.agent(n, p, int(k[0]), int(k[1]), s) #simulate agent data using corresponding param
                #EndPolicy[n-1,s,:] = endPolicy
                Data = np.vstack((Data, sub))
        f.np2pd(Data[1:], self.colnamesOut).to_csv(r'../Learning_Validation/' + self.name + '.csv', index=False, header=True)
        #np.savez(r'../Model_Comparison&Simulation/EndQPolicy' + self.name, EndQPolicy = EndPolicy, agtName=self.name)
    def Get_EndPolicy(self):
        '''
        :param Param: The simulating parameters of all subjects (list of lists)
        :param Kap: WM capacity of all subjects (int list)
        :param nSim: number of simulation per subject
        :return: a pandas dataframe
        '''
        print('now geting the EndPolicy of ' + self.name)
        ModFit = np.load('../ModelFit/ModFit' + self.name + '.npz',
                         allow_pickle=True)  # loads your saved array into variable a.
        Param, Kap = ModFit['Params'], ModFit['Ks']
        EndPolicy = np.empty((len(self.Sub), len(self.Block)), dtype=list)
        for n in self.Sub:  # for every subject
            print('subject ' + str(n))
            p, k = Param[n - 1], Kap[n - 1]  # get the subject's parameter values
            sub, EndPolicy[n - 1, :] = self.fake_agent(n, p, int(k[0]), int(k[1]))  # simulate agent data using corresponding param
        np.savez(r'../Model_Comparison&Simulation/EndQPolicy' + self.name, EndQPolicy=EndPolicy, agtName=self.name)
        return EndPolicy
    def modFit(self, initSub=1, endSub=33, niter=20):
        '''
        Model Fitting: Saves the ICs, Parameters, etc as an npz file after fitting every subject
        :param initSub: The subject to start fitting
        :param endSub: The subject to fit until
        :param niter: number of iterations for graient descent optimizer
        :return: the runtime but will create .npz file to save the parameters
        '''
        start_time = time.time() #register the start time
        nSub = len(self.Sub) #number of subjects
        Data = self.data #get the data
        print('now fitting ' + self.name +' make sure your computer is Caffeinated')
        for sub in range(initSub, endSub + 1):  # for every subject data of interest
            print('subject: ' + str(sub))
            data = f.pd2np(Data[Data['subject'] == sub][self.colnamesOut])  # get the data for individual sub and convert to numpy
            try:  # see if there is a existing saved file
                ModFit = np.load('ModFit' + self.name + '.npz', allow_pickle=True)  # if no error returned
            except:  # if no existing file, np.load would return error and so we initialize a file.
                AICs, BICs, Ks, niters, Params = np.empty(nSub), np.empty(nSub), np.empty(nSub, dtype=tuple),\
                                         np.empty(nSub), np.empty(nSub,dtype=list) # Ks will be tuples K[0] is the best fitting K, K[1] is the KapDiscount
            else:
                AICs, BICs, Ks, niters, Params = ModFit['AICs'], ModFit['BICs'], ModFit['Ks'],ModFit['niters'], ModFit['Params']  # get existing fitted values
            niters[sub-1] = niter
            AICs[sub - 1], BICs[sub - 1], Params[sub - 1], Ks[sub - 1] = f.opt2IC(self.LLH, self.numParam, data, K = self.Krange, Dis = self.KapDiscount, niter=niter)  # fit the currect subject
            np.savez('ModFit' + self.name, AICs=AICs, BICs=BICs, Ks=Ks, Params=Params, niters=niters,agtName=self.name)  # save the file including the current subject data
        print("--- %s seconds ---" % (time.time() - start_time))
    '''
    Testing Data Simulation and Fitting
    '''
    def te_agent(self, subNum, param, endPolicy, na=3):
        '''
        Simulate the testing data for one subject
        :param subNum: The subject number
        :param endPolicy: The RL policy at the end of the learning. List of matrices for each block
        :param na: number of action. Defaut 3
        :return: a testing data set simulated
        '''
        retentionST, retentionDT = param
        # specifying indices
        bIdx = self.colnamesIn_te.index('block')
        nsIdx = self.colnamesIn_te.index('ns')
        dtIdx = self.colnamesIn_te.index('dt')
        corActIdx = self.colnamesIn_te.index('corAct')
        stimSeqIdx = self.colnamesIn_te.index('stimSeq')
        # get the subject data
        te_data = self.subject_data_te[subNum - 1]
        # initialize output data
        te_Data = np.ones(len(self.colnamesOut_te))
        #array of blocks and a uniform policy
        B, U = np.unique(te_data[:, bIdx]), np.full(na, 1 / na)
        row, nrow = 0, np.size(te_data, 0) #get number of rows
        for b in B: # loop blocks
            ns, dt = int(te_data[row, nsIdx]), int(te_data[row, dtIdx]) # get ns and dt of the block
            retention = (dt==0)*retentionST + (dt==1)*retentionDT
            policy = endPolicy[int(b)-1] #find the policy matrix of corresponding
            while row<nrow and int(te_data[row, bIdx]) == b: #for each row of this particular block
                stim, corAct = int(te_data[row, stimSeqIdx]), int(te_data[row, corActIdx])
                #use policy after retention to select testing action of the corresponding stimulus
                act = np.random.choice(range(na), p=retention*policy[stim-1] + (1-retention)*U)
                cor = act+1 == corAct
                # stack data
                te_Data = np.vstack((te_Data, np.array([subNum, b, dt, ns, act, cor, stim])))
                row += 1 #got to next row
        return te_Data[1:]# remove first row
    def te_llh(self, te_data, endPolicy = (),param = (1,1), na=3):
        '''
        computing the llh of testing data
        :param param: unary list of only the retention parameter
        :param te_data: te_data of current subject
        :param endPolicy: The RL policy at the end of the learning. List of matrices for each block
        :return: a testing data set simulated
        '''
        # specifying indices
        retentionST, retentionDT = param
        bIdx = self.colnamesOut_te.index('block')
        dtIdx = self.colnamesOut_te.index('dt')
        corIdx = self.colnamesOut_te.index('Cor')
        actionIdx = self.colnamesOut_te.index('action')
        stimSeqIdx = self.colnamesOut_te.index('stimSeq')
        #array of blocks and a uniform policy
        llh, row, nrow, B = 0, 0, np.size(te_data, 0), np.unique(te_data[:, bIdx])
        for b in B: # loop blocks
            policy = endPolicy[int(b)-1] # find the policy matrix of corresponding block
            dt = int(te_data[row, dtIdx])
            retention = (dt == 0) * retentionST + (dt == 1) * retentionDT
            while row<nrow and int(te_data[row, bIdx]) == b: # for each row of this particular block
                stim, Act = int(te_data[row, stimSeqIdx]), int(te_data[row, actionIdx])-1
                #use policy after retention to select testing action of the corresponding stimulus
                llh += np.log(retention*policy[stim-1, Act] + (1-retention)/na)
                row += 1 #got to next row
        return -llh
    def te_IC(self, initSub=1, endSub=33):
        '''
        Model Fitting: Saves the ICs, Parameters, etc as an npz file after fitting every subject
        :param initSub: The subject to start fitting
        :param endSub: The subject to fit until
        :param niter: number of iterations for graient descent optimizer
        :return: the runtime but will create .npz file to save the parameters
        '''
        nSub = len(self.Sub) #number of subjects
        te_Data = self.te_data #get the data
        EndQPolicy = np.load('../Model_Comparison&Simulation/EndQPolicy' + self.name + '.npz', allow_pickle=True)
        EndPolicy = EndQPolicy['EndQPolicy']
        print('now fitting ' + self.name)
        AICs, BICs = np.empty(nSub), np.empty(nSub)
        for sub in range(initSub, endSub + 1):  # for every subject data of interest
            endPolicy = EndPolicy[sub-1]
            te_data = f.pd2np(te_Data[te_Data['subject'] == sub][self.colnamesOut_te])  # get the data for individual sub and convert to numpy
            bestllh = self.te_llh(te_data, endPolicy)
            print(bestllh)
            AICs[sub - 1], BICs[sub - 1] = f.get_AIC(bestllh, self.numParam), f.get_BIC(bestllh, self.numParam, te_data.shape[0])
        np.savez('Testing_ModelFit' + self.name, AICs=AICs, BICs=BICs, agtName=self.name)  # save the file including the current subject data
    def te_Sim_agent(self, nSim=100):
        '''
        :param Param: The simulating parameters of all subjects (list of lists)
        :param Kap: WM capacity of all subjects (int list)
        :param nSim: number of simulation per subject
        :return: a pandas dataframe
        '''
        print('now simulating the testing data of '+ self.name)
        te_Data = np.ones(len(self.colnamesOut_te))
        te_ModFit = np.load('../Testing_ModelFit/Testing_ModelFit' + self.name + '.npz', allow_pickle=True)
        Params = [1,1]#te_ModFit['Params']
        EndQPolicy = np.load('../Model_Comparison&Simulation/EndQPolicy' + self.name + '.npz', allow_pickle=True)
        EndPolicy = EndQPolicy['EndQPolicy']
        for n in self.Sub:#for every subject
            print('subject ' + str(n))
            endPolicy = EndPolicy[n-1]
            for s in range(nSim): #for every simulation
                sub_te = self.te_agent(n, Params, endPolicy)
                te_Data = np.vstack((te_Data, sub_te))
        f.np2pd(te_Data[1:], self.colnamesOut_te).to_csv(r'../Testing_Validation/' + self.name + '.csv',
                                                         index=False, header=True)
    def te_modFit(self, initSub=1, endSub=33, niter=20):
        '''
        Model Fitting: Saves the ICs, Parameters, etc as an npz file after fitting every subject
        :param initSub: The subject to start fitting
        :param endSub: The subject to fit until
        :param niter: number of iterations for graient descent optimizer
        :return: the runtime but will create .npz file to save the parameters
        '''
        start_time = time.time() #register the start time
        nSub = len(self.Sub) #number of subjects
        te_Data = self.te_data #get the data
        EndQPolicy = np.load('../Model_Comparison&Simulation/EndQPolicy' + self.name + '.npz', allow_pickle=True)
        EndPolicy = EndQPolicy['EndQPolicy']
        print('now fitting ' + self.name +' make sure your computer is Caffeinated')
        for sub in range(initSub, endSub + 1):  # for every subject data of interest
            print('subject: ' + str(sub))
            endPolicy = EndPolicy[sub-1]
            te_data = f.pd2np(te_Data[te_Data['subject'] == sub][self.colnamesOut_te])  # get the data for individual sub and convert to numpy
            nSim = np.size(endPolicy, 0)
            for sim in range(nSim):
                try:  # see if there is a existing saved file
                    ModFit = np.load('Testing_ModelFit' + self.name + '.npz', allow_pickle=True)  # if no error returned
                except:  # if no existing file, np.load would return error and so we initialize a file.
                    AICs, BICs, niters, Params = np.empty((nSub, nSim)), np.empty((nSub, nSim)),\
                                             np.empty((nSub, nSim)), np.empty((nSub, nSim),dtype=list) # Ks will be tuples K[0] is the best fitting K, K[1] is the KapDiscount
                else:
                    AICs, BICs, niters, Params = ModFit['AICs'], ModFit['BICs'],ModFit['niters'], ModFit['Params']  # get existing fitted values
                niters[sub-1, sim] = niter
                AICs[sub - 1, sim], BICs[sub - 1, sim], Params[sub - 1, sim] = f.opt2IC_te(self.te_llh, self.te_numParams, self.numParam, te_data, endPolicy[sim], niter=niter)  # fit the currect subject
                np.savez('Testing_ModelFit' + self.name, AICs=AICs, BICs=BICs, Params=Params, niters=niters, agtName=self.name)  # save the file including the current subject data
        print("--- %s seconds ---" % (time.time() - start_time))


'''
Baseline Models
'''
class RLWM_noneFree(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_noneFree'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta']
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na), np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, stim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na), np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap=3, KapDiscount = 1, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    initPolicy = weight * f.softmax(W[stim - 1], beta) + \
                                 (1 - weight) * f.softmax(Q[stim - 1], beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1, a]), (Cor - W[stim - 1, a])
                    Q[stim - 1, a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1, a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh
class RLWM_allFree(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_allFree'
        self.pname = ['etaST', 'forgetST', 'rhoST', 'noiseST', 'neg_etaST', 'etaDT', 'forgetDT', 'rhoDT', 'noiseDT', 'neg_etaDT']
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        etaST = param[self.pname.index('etaST')]  # learning rate
        forgetST = param[self.pname.index('forgetST')]  # WM forgetting
        rhoST = param[self.pname.index('rhoST')]  # initial WM weight
        noiseST = param[self.pname.index('noiseST')]
        neg_etaST = param[self.pname.index('neg_etaST')]  # perserverance weight to learning rate in negative feedback
        etaDT = param[self.pname.index('etaDT')]  # learning rate
        forgetDT = param[self.pname.index('forgetDT')]  # WM forgetting
        rhoDT = param[self.pname.index('rhoDT')]  # initial WM weight
        noiseDT = param[self.pname.index('noiseDT')]
        neg_etaDT = param[self.pname.index('neg_etaDT')]  # perserverance weight to learning rate in negative feedback
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            eta = (dt==0)*etaST + (dt==1)*etaDT
            forget = (dt==0)*forgetST + (dt==1)*forgetDT
            rho = (dt==0)*rhoST + (dt==1)*rhoDT
            noise = (dt==0)*noiseST + (dt==1)*noiseDT
            neg_eta = (dt==0)*neg_etaST + (dt==1)*neg_etaDT
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na), np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, stim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        etaST = param[self.pname.index('etaST')]  # learning rate
        forgetST = param[self.pname.index('forgetST')]  # WM forgetting
        rhoST = param[self.pname.index('rhoST')]  # initial WM weight
        noiseST = param[self.pname.index('noiseST')]
        neg_etaST = param[self.pname.index('neg_etaST')]  # perserverance weight to learning rate in negative feedback
        etaDT = param[self.pname.index('etaDT')]  # learning rate
        forgetDT = param[self.pname.index('forgetDT')]  # WM forgetting
        rhoDT = param[self.pname.index('rhoDT')]  # initial WM weight
        noiseDT = param[self.pname.index('noiseDT')]
        neg_etaDT = param[self.pname.index('neg_etaDT')]  # perserverance weight to learning rate in negative feedback
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            eta = (dt==0)*etaST + (dt==1)*etaDT
            forget = (dt==0)*forgetST + (dt==1)*forgetDT
            rho = (dt==0)*rhoST + (dt==1)*rhoDT
            noise = (dt==0)*noiseST + (dt==1)*noiseDT
            neg_eta = (dt==0)*neg_etaST + (dt==1)*neg_etaDT
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na), np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap=3, KapDiscount = 1, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        etaST = param[self.pname.index('etaST')]  # learning rate
        forgetST = param[self.pname.index('forgetST')]  # WM forgetting
        rhoST = param[self.pname.index('rhoST')]  # initial WM weight
        noiseST = param[self.pname.index('noiseST')]
        neg_etaST = param[self.pname.index('neg_etaST')]  # perserverance weight to learning rate in negative feedback
        etaDT = param[self.pname.index('etaDT')]  # learning rate
        forgetDT = param[self.pname.index('forgetDT')]  # WM forgetting
        rhoDT = param[self.pname.index('rhoDT')]  # initial WM weight
        noiseDT = param[self.pname.index('noiseDT')]
        neg_etaDT = param[self.pname.index('neg_etaDT')]  # perserverance weight to learning rate in negative feedback
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            eta = (dt==0)*etaST + (dt==1)*etaDT
            forget = (dt==0)*forgetST + (dt==1)*forgetDT
            rho = (dt==0)*rhoST + (dt==1)*rhoDT
            noise = (dt==0)*noiseST + (dt==1)*noiseDT
            neg_eta = (dt==0)*neg_etaST + (dt==1)*neg_etaDT
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    initPolicy = weight * f.softmax(W[stim - 1], beta) + \
                                 (1 - weight) * f.softmax(Q[stim - 1], beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1, a]), (Cor - W[stim - 1, a])
                    Q[stim - 1, a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1, a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh
class RLWM_noise(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_noise'
        self.pname = ['eta', 'forget', 'rho', 'noiseST', 'noiseDT', 'neg_eta']
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noiseST = param[self.pname.index('noiseST')]
        noiseDT = param[self.pname.index('noiseDT')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            noise = (dt==0)*noiseST + (dt==1)*noiseDT
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na), np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, stim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noiseST = param[self.pname.index('noiseST')]
        noiseDT = param[self.pname.index('noiseDT')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            noise = (dt==0)*noiseST + (dt==1)*noiseDT
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na), np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap=3, KapDiscount = 1, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noiseST = param[self.pname.index('noiseST')]
        noiseDT = param[self.pname.index('noiseDT')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            noise = (dt == 0) * noiseST + (dt == 1) * noiseDT
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    initPolicy = weight * f.softmax(W[stim - 1], beta) + \
                                 (1 - weight) * f.softmax(Q[stim - 1], beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1, a]), (Cor - W[stim - 1, a])
                    Q[stim - 1, a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1, a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh
class RLWM_modulation(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_modulation'
        self.pname = ['etaST','etaDT','forget', 'rho', 'noise', 'neg_eta']
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        etaST = param[self.pname.index('etaST')]  # learning rate
        etaDT = param[self.pname.index('etaDT')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            eta = (dt==0)*etaST + (dt==1)*etaDT
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na), np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, stim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        etaST = param[self.pname.index('etaST')]  # learning rate
        etaDT = param[self.pname.index('etaDT')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            eta = (dt==0)*etaST + (dt==1)*etaDT
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na), np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap=3, KapDiscount = 1, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        etaST = param[self.pname.index('etaST')]  # learning rate
        etaDT = param[self.pname.index('etaDT')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            eta = (dt==0)*etaST + (dt==1)*etaDT
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    initPolicy = weight * f.softmax(W[stim - 1], beta) + \
                                 (1 - weight) * f.softmax(Q[stim - 1], beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1, a]), (Cor - W[stim - 1, a])
                    Q[stim - 1, a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1, a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

'''
Credit Assignment Models
'''
class RLWM_credit(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_credit'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'lambdaST', 'lambdaDT']
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na), np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # wrong credit assignment Q-learning
                for k in range(ns):
                    if k +1 != stim:
                        Q[k,a] += lambdaT * (neg_eta * (Cor == 0) + (Cor == 1)) * eta * (Cor - Q[k,a])
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, stim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na), np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # wrong credit assignment Q-learning
                for k in range(ns):
                    if k +1 != stim:
                        Q[k,a] += lambdaT * (neg_eta * (Cor == 0) + (Cor == 1)) * eta * (Cor - Q[k,a])
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    initPolicy = weight * f.softmax(W[stim - 1], beta) + \
                                 (1 - weight) * f.softmax(Q[stim - 1], beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1, a]), (Cor - W[stim - 1, a])
                    Q[stim - 1, a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1, a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # wrong credit assignment Q-learning
                    for k in range(ns):
                        if k + 1 != stim:
                            Q[k, a] += lambdaT * (neg_eta * (Cor == 0) + (Cor == 1)) * eta * (Cor - Q[k, a])
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

class RLWM_credit_i(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_credit_i'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'lambdaST', 'lambdaDT']
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        Choice = np.empty(len(B), dtype=list)
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # wrong credit assignment Q-learning
                for k in range(ns):
                    if k +1 != stim:
                        Q[k,a] += lambdaT * (neg_eta * (Cor == 0) + (Cor == 1)) * eta * (Cor - Q[k,a])
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, stim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # wrong credit assignment Q-learning
                for k in range(ns):
                    if k +1 != stim:
                        Q[k,a] += lambdaT * (neg_eta * (Cor == 0) + (Cor == 1)) * eta * (Cor - Q[k,a])
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap=3, KapDiscount=1, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(Q[stim - 1], beta)
                    initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space:
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # wrong credit assignment Q-learning
                    for k in range(ns):
                        if k + 1 != stim:
                            Q[k,a] += lambdaT * (neg_eta * (Cor == 0) + (Cor == 1)) * eta * (Cor - Q[k,a])
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

class RLWM_credit_ii(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_credit_ii'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'lambdaST', 'lambdaDT']
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space:
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # wrong credit assignment Q-learning
                for k in range(ns):
                    if k +1 != stim:
                        Q[k,a] += lambdaT * (neg_eta * (Cor == 0) + (Cor == 1)) * eta * (Cor - (weight*W[k,a] + (1-weight)*Q[k,a]))
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, stim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space:
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # wrong credit assignment Q-learning
                for k in range(ns):
                    if k +1 != stim:
                        Q[k,a] += lambdaT * (neg_eta * (Cor == 0) + (Cor == 1)) * eta * (Cor - (weight*W[k,a] + (1-weight)*Q[k,a]))
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    initPolicy = weight * f.softmax(W[stim - 1], beta) + \
                                 (1 - weight) * f.softmax(Q[stim - 1], beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # wrong credit assignment Q-learning
                    for k in range(ns):
                        if k + 1 != stim:
                            Q[k,a] += lambdaT * (neg_eta * (Cor == 0) + (Cor == 1)) * eta * (Cor - (weight*W[k,a] + (1-weight)*Q[k,a]))
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

'''
Action Assignment Models
'''
class RLWM_action_DisFree(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_action_DisFree'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'lambdaST', 'lambdaDT']
        self.KapDiscount = (1, 2)
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, stim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap- KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                    initPolicy = weight * f.softmax(W[stim - 1], beta) + \
                              (1 - weight) * f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

class RLWM_action(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_action'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'lambdaST', 'lambdaDT']
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, stim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                    initPolicy = weight * f.softmax(W[stim - 1], beta) + \
                              (1 - weight) * f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

class RLWM_action_i(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_action_i'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'lambdaST', 'lambdaDT']
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, stim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                    initPolicy = weight * f.softmax(W[stim - 1], beta) + \
                              (1 - weight) * f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

class RLWM_actionSoftmax(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_actionSoftmax'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'lambdaST', 'lambdaDT']
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), (lambdaT*f.softmax(Qwrong, beta) + (1-lambdaT)*f.softmax(Q[stim - 1], beta))
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, stim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), (lambdaT*f.softmax(Qwrong, beta) + (1-lambdaT)*f.softmax(Q[stim - 1], beta))
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                    initPolicy = weight * f.softmax(W[stim - 1], beta) + \
                                 (1 - weight) * (lambdaT * f.softmax(Qwrong, beta) +
                                                 (1 - lambdaT) * f.softmax(Q[stim - 1],beta))  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

class RLWM_actionWQ(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_actionWQ'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'lambdaST', 'lambdaDT']
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wwrong = np.mean(np.delete(W, stim - 1, axis=0), axis=0)
                Wpolicy, Qpolicy = f.softmax(lambdaT*Wwrong + (1-lambdaT)*W[stim - 1], beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, stim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wwrong = np.mean(np.delete(W, stim - 1, axis=0), axis=0)
                Wpolicy, Qpolicy = f.softmax(lambdaT*Wwrong + (1-lambdaT)*W[stim - 1], beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),
                                     axis=0)  # average the Q values that do not concern the current stimulus
                    Wwrong = np.mean(np.delete(W, stim - 1, axis=0), axis=0)
                    initPolicy = weight * f.softmax(lambdaT * Wwrong + (1 - lambdaT) * W[stim - 1], beta) + \
                                 (1 - weight) * f.softmax(lambdaT * Qwrong + (1 - lambdaT) * Q[stim - 1],
                                                          beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

class RLWM_actionWQ2(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_actionWQ2'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'lambdaST', 'lambdaDT', 'lambdaW']
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        lambdaW = param[self.pname.index('lambdaW')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wwrong = np.mean(np.delete(W, stim - 1, axis=0), axis=0)
                Wpolicy, Qpolicy = f.softmax(lambdaW*Wwrong + (1-lambdaW)*W[stim - 1], beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, stim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        lambdaW = param[self.pname.index('lambdaW')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wwrong = np.mean(np.delete(W, stim - 1, axis=0), axis=0)
                Wpolicy, Qpolicy = f.softmax(lambdaW*Wwrong + (1-lambdaW)*W[stim - 1], beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        lambdaW = param[self.pname.index('lambdaW')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),
                                     axis=0)  # average the Q values that do not concern the current stimulus
                    Wwrong = np.mean(np.delete(W, stim - 1, axis=0), axis=0)
                    initPolicy = weight * f.softmax(lambdaW * Wwrong + (1 - lambdaW) * W[stim - 1], beta) + \
                                 (1 - weight) * f.softmax(lambdaT * Qwrong + (1 - lambdaT) * Q[stim - 1],
                                                          beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

'''
Action and Credit Assignment Mixture Models
'''
class RLWM_credAct_i(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_credAct_i'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'lambdaST', 'lambdaDT', 'lambda_act']
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        lambda_act = param[self.pname.index('lambda_act')] #wrong action assigment
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(lambda_act*Qwrong + (1-lambda_act)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # wrong credit assignment Q-learning
                for k in range(ns):
                    if k +1 != stim:
                        Q[k,a] += lambdaT * (neg_eta * (Cor == 0) + (Cor == 1)) * eta * (Cor - Q[k,a])
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, stim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        lambda_act = param[self.pname.index('lambda_act')] #wrong action assigment
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(lambda_act*Qwrong + (1-lambda_act)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # wrong credit assignment Q-learning
                for k in range(ns):
                    if k +1 != stim:
                        Q[k,a] += lambdaT * (neg_eta * (Cor == 0) + (Cor == 1)) * eta * (Cor - Q[k,a])
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        lambda_act = param[self.pname.index('lambda_act')] #wrong action assigment
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),
                                     axis=0)  # average the Q values that do not concern the current stimulus
                    initPolicy = weight * f.softmax(W[stim - 1], beta) + \
                                 (1 - weight) * f.softmax(lambda_act * Qwrong + (1 - lambda_act) * Q[stim - 1],
                                                          beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # wrong credit assignment Q-learning
                    for k in range(ns):
                        if k + 1 != stim:
                            Q[k,a] += lambdaT * (neg_eta * (Cor == 0) + (Cor == 1)) * eta * (Cor - Q[k,a])
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

class RLWM_credActDouble_i(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_credActDouble_i'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'lambdaST', 'lambdaDT', 'ActST', 'ActDT']
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        ActST = param[self.pname.index('ActST')] #wrong action assigment
        ActDT = param[self.pname.index('ActDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            ActT = (dt == 1) * ActDT + (dt == 0) * ActST  # wrong action assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(ActT*Qwrong + (1-ActT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # wrong credit assignment Q-learning
                for k in range(ns):
                    if k +1 != stim:
                        Q[k,a] += lambdaT * (neg_eta * (Cor == 0) + (Cor == 1)) * eta * (Cor - Q[k,a])
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        ActST = param[self.pname.index('ActST')] #wrong action assigment
        ActDT = param[self.pname.index('ActDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            ActT = (dt == 1) * ActDT + (dt == 0) * ActST  # wrong action assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                Qwrong = np.mean(np.delete(Q, stim-1, axis=0), axis=0) #average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(ActT*Qwrong + (1-ActT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # wrong credit assignment Q-learning
                for k in range(ns):
                    if k +1 != stim:
                        Q[k,a] += lambdaT * (neg_eta * (Cor == 0) + (Cor == 1)) * eta * (Cor - Q[k,a])
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        ActST = param[self.pname.index('ActST')]  # wrong action assigment
        ActDT = param[self.pname.index('ActDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            ActT = (dt == 1) * ActDT + (dt == 0) * ActST  # wrong action assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),
                                     axis=0)  # average the Q values that do not concern the current stimulus
                    initPolicy = weight * f.softmax(W[stim - 1], beta) + \
                                 (1 - weight) * f.softmax(ActT * Qwrong + (1 - ActT) * Q[stim - 1],
                                                          beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # wrong credit assignment Q-learning
                    for k in range(ns):
                        if k + 1 != stim:
                            Q[k,a] += lambdaT * (neg_eta * (Cor == 0) + (Cor == 1)) * eta * (Cor - Q[k,a])
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

'''
Attention Filter Models
'''

class RLWM_filter(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_filter'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'beta_filterST', 'beta_filterDT']
        self.filter_beta_scale = 50
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filterST = self.filter_beta_scale*param[self.pname.index('beta_filterST')]  # filtered beta
        beta_filterDT = self.filter_beta_scale*param[self.pname.index('beta_filterDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            beta_filter = (dt == 1) * beta_filterDT + (dt == 0) * beta_filterST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                stim_vec = np.zeros(ns)  # attention over all stimuli
                stim_vec[stim - 1] = 1  # certain about the current stimulus
                stim_filtered = f.softmax(stim_vec, beta_filter)  # not so much. filtered stimulus
                Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta),f.softmax(stim_filtered.dot(Q),beta)
                initPolicy = weight * Wpolicy + (1 - weight) *  Qpolicy # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filterST = self.filter_beta_scale*param[self.pname.index('beta_filterST')]  # filtered beta
        beta_filterDT = self.filter_beta_scale*param[self.pname.index('beta_filterDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            beta_filter = (dt == 1) * beta_filterDT + (dt == 0) * beta_filterST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                stim_vec = np.zeros(ns)  # attention over all stimuli
                stim_vec[stim - 1] = 1  # certain about the current stimulus
                stim_filtered = f.softmax(stim_vec, beta_filter)  # not so much. filtered stimulus
                Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta),f.softmax(stim_filtered.dot(Q),beta)
                initPolicy = weight * Wpolicy + (1 - weight) *  Qpolicy # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filterST = self.filter_beta_scale*param[self.pname.index('beta_filterST')]  # filtered beta
        beta_filterDT = self.filter_beta_scale*param[self.pname.index('beta_filterDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            beta_filter = (dt == 1) * beta_filterDT + (dt == 0) * beta_filterST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    stim_vec = np.zeros(ns) #attention over all stimuli
                    stim_vec[stim-1] = 1 #certain about the current stimulus
                    stim_filtered = f.softmax(stim_vec, beta_filter) #not so much. filtered stimulus
                    initPolicy = weight * f.softmax(stim_filtered.dot(W), beta) + \
                                 (1 - weight) * f.softmax(stim_filtered.dot(Q), beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

class RLWM_filterQ(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_filterQ'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'beta_filterST', 'beta_filterDT']
        self.filter_beta_scale = 50
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filterST = self.filter_beta_scale*param[self.pname.index('beta_filterST')]  # filtered beta
        beta_filterDT = self.filter_beta_scale*param[self.pname.index('beta_filterDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            beta_filter = (dt == 1) * beta_filterDT + (dt == 0) * beta_filterST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                stim_vec = np.zeros(ns)  # attention over all stimuli
                stim_vec[stim - 1] = 1  # certain about the current stimulus
                stim_filtered = f.softmax(stim_vec, beta_filter)  # not so much. filtered stimulus
                Wpolicy, Qpolicy = f.softmax(W[stim-1], beta), f.softmax(stim_filtered.dot(Q),beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filterST = self.filter_beta_scale*param[self.pname.index('beta_filterST')]  # filtered beta
        beta_filterDT = self.filter_beta_scale*param[self.pname.index('beta_filterDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')

        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            beta_filter = (dt == 1) * beta_filterDT + (dt == 0) * beta_filterST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                stim_vec = np.zeros(ns)  # attention over all stimuli
                stim_vec[stim - 1] = 1  # certain about the current stimulus
                stim_filtered = f.softmax(stim_vec, beta_filter)  # not so much. filtered stimulus
                Wpolicy, Qpolicy = f.softmax(W[stim-1], beta), f.softmax(stim_filtered.dot(Q),beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filterST = self.filter_beta_scale*param[self.pname.index('beta_filterST')]  # filtered beta
        beta_filterDT = self.filter_beta_scale*param[self.pname.index('beta_filterDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            beta_filter = (dt == 1) * beta_filterDT + (dt == 0) * beta_filterST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    stim_vec = np.zeros(ns) #attention over all stimuli
                    stim_vec[stim-1] = 1 #certain about the current stimulus
                    stim_filtered = f.softmax(stim_vec, beta_filter) #not so much. filtered stimulus
                    initPolicy = weight * f.softmax(W[stim-1], beta) + \
                                 (1 - weight) * f.softmax(stim_filtered.dot(Q), beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

class RLWM_filterW(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_filterW'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'beta_filterST', 'beta_filterDT']
        self.filter_beta_scale = 50
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount,sim=1, beta=50, na=3):
            '''
            :param subNum: The subject number of the agent
            :param param: The corresponding parameters
            :param Kap: The corresponding capacity
            :param beta: The beta for softmax
            :param na: The number of actions
            :return: numpy data matrix of simulated data
            '''
            # model parameters
            eta = param[self.pname.index('eta')]  # learning rate
            forget = param[self.pname.index('forget')]  # WM forgetting
            rho = param[self.pname.index('rho')]  # initial WM weight
            noise = param[self.pname.index('noise')]
            neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
            beta_filterST = self.filter_beta_scale * param[self.pname.index('beta_filterST')]  # filtered beta
            beta_filterDT = self.filter_beta_scale * param[self.pname.index('beta_filterDT')]
            # get coloumn index for different task variables
            bIdx = self.colnamesIn.index('block')
            nsIdx = self.colnamesIn.index('ns')
            dtIdx = self.colnamesIn.index('dt')
            actionIdx = self.colnamesIn.index('action')
            corActIdx = self.colnamesIn.index('corAct')
            stimSeqIdx = self.colnamesIn.index('stimSeq')
            numTrialIdx = self.colnamesIn.index('numTrial')
            # prep work
            data = self.subject_data[subNum - 1]  # get the subject data
            nrow = np.size(data, 0)  # get number of rows
            Data = np.ones(len(self.colnamesOut))  # initialize output data
            U = np.full(na, 1 / na)  # define uniform policy for later use
            B = np.unique(data[:, bIdx])
            EndPolicy = np.empty(len(B), dtype=list)  # store the end policies of each block
            row = 0
            for b in B:  # loop blocks
                ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx])  # get ns and dt of the block
                # get param for specific dt condition
                beta_filter = (dt == 1) * beta_filterDT + (dt == 0) * beta_filterST  # wrong credit assignment weight
                K = (dt == 1) * (Kap - KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
                weight = rho * min(1, K / ns)  # working memory weight
                # initialize state space value matrices
                Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na), np.empty((ns, na))
                while row < nrow and int(data[row, bIdx]) == b:  # for each row of this particular block
                    corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(
                        data[row, numTrialIdx])
                    # compute softmax probabilities
                    stim_vec = np.zeros(ns)  # attention over all stimuli
                    stim_vec[stim - 1] = 1  # certain about the current stimulus
                    stim_filtered = f.softmax(stim_vec, beta_filter)  # not so much. filtered stimulus
                    Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta), f.softmax(Q[stim - 1], beta)
                    initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    endPolicy[stim - 1] = Qpolicy
                    # pick action
                    try:
                        a = int(data[row, actionIdx])-1
                    except:
                        row += 1
                        continue
                    # correct action is
                    Cor = a + 1 == corAct
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1, a]), (Cor - W[stim - 1, a])
                    Q[stim - 1, a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1, a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1 / na - W)
                    # stack data
                    Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                    row += 1  # got to next row
                    EndPolicy[int(b) - 1] = endPolicy
            return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filterST = self.filter_beta_scale*param[self.pname.index('beta_filterST')]  # filtered beta
        beta_filterDT = self.filter_beta_scale*param[self.pname.index('beta_filterDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            beta_filter = (dt == 1) * beta_filterDT + (dt == 0) * beta_filterST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                stim_vec = np.zeros(ns)  # attention over all stimuli
                stim_vec[stim - 1] = 1  # certain about the current stimulus
                stim_filtered = f.softmax(stim_vec, beta_filter)  # not so much. filtered stimulus
                Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta), f.softmax(Q[stim-1], beta)
                initPolicy = weight * Wpolicy +  (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filterST = self.filter_beta_scale*param[self.pname.index('beta_filterST')]  # filtered beta
        beta_filterDT = self.filter_beta_scale*param[self.pname.index('beta_filterDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            beta_filter = (dt == 1) * beta_filterDT + (dt == 0) * beta_filterST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    stim_vec = np.zeros(ns) #attention over all stimuli
                    stim_vec[stim-1] = 1 #certain about the current stimulus
                    stim_filtered = f.softmax(stim_vec, beta_filter) #not so much. filtered stimulus
                    initPolicy = weight * f.softmax(stim_filtered.dot(W), beta) + \
                                 (1 - weight) * f.softmax(Q[stim-1], beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

'''
Attention Filter and Action Assignment Mixture Models
'''

class RLWM_WFQA(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_WFQA'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'beta_filterST', 'beta_filterDT', 'lambdaST', 'lambdaDT']
        self.filter_beta_scale = 50
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filterST = self.filter_beta_scale*param[self.pname.index('beta_filterST')]  # filtered beta
        beta_filterDT = self.filter_beta_scale*param[self.pname.index('beta_filterDT')]
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            beta_filter = (dt == 1) * beta_filterDT + (dt == 0) * beta_filterST  # wrong credit assignment weight
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                stim_vec = np.zeros(ns)  # attention over all stimuli
                stim_vec[stim - 1] = 1  # certain about the current stimulus
                stim_filtered = f.softmax(stim_vec, beta_filter)  # not so much. filtered stimulus
                Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),axis=0)  # average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filterST = self.filter_beta_scale*param[self.pname.index('beta_filterST')]  # filtered beta
        beta_filterDT = self.filter_beta_scale*param[self.pname.index('beta_filterDT')]
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            beta_filter = (dt == 1) * beta_filterDT + (dt == 0) * beta_filterST  # wrong credit assignment weight
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                stim_vec = np.zeros(ns)  # attention over all stimuli
                stim_vec[stim - 1] = 1  # certain about the current stimulus
                stim_filtered = f.softmax(stim_vec, beta_filter)  # not so much. filtered stimulus
                Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),axis=0)  # average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filterST = self.filter_beta_scale*param[self.pname.index('beta_filterST')]  # filtered beta
        beta_filterDT = self.filter_beta_scale*param[self.pname.index('beta_filterDT')]
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            beta_filter = (dt == 1) * beta_filterDT + (dt == 0) * beta_filterST  # wrong credit assignment weight
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    stim_vec = np.zeros(ns) #attention over all stimuli
                    stim_vec[stim-1] = 1 #certain about the current stimulus
                    stim_filtered = f.softmax(stim_vec, beta_filter) #not so much. filtered stimulus
                    Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),
                                     axis=0)  # average the Q values that do not concern the current stimulus
                    initPolicy = weight * f.softmax(stim_filtered.dot(W), beta) + \
                                 (1 - weight) * f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

class RLWM_WFQA2(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_WFQA2'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'beta_filter', 'lambdaST', 'lambdaDT']
        self.filter_beta_scale = 50
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount,sim=1, beta=50, na=3):
            '''
            :param subNum: The subject number of the agent
            :param param: The corresponding parameters
            :param Kap: The corresponding capacity
            :param beta: The beta for softmax
            :param na: The number of actions
            :return: numpy data matrix of simulated data
            '''
            # model parameters
            eta = param[self.pname.index('eta')]  # learning rate
            forget = param[self.pname.index('forget')]  # WM forgetting
            rho = param[self.pname.index('rho')]  # initial WM weight
            noise = param[self.pname.index('noise')]
            neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
            beta_filter = self.filter_beta_scale * param[self.pname.index('beta_filter')]  # filtered beta
            lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
            lambdaDT = param[self.pname.index('lambdaDT')]
            # get coloumn index for different task variables
            bIdx = self.colnamesIn.index('block')
            nsIdx = self.colnamesIn.index('ns')
            dtIdx = self.colnamesIn.index('dt')
            actionIdx = self.colnamesIn.index('action')
            corActIdx = self.colnamesIn.index('corAct')
            stimSeqIdx = self.colnamesIn.index('stimSeq')
            numTrialIdx = self.colnamesIn.index('numTrial')
            # prep work
            data = self.subject_data[subNum - 1]  # get the subject data
            nrow = np.size(data, 0)  # get number of rows
            Data = np.ones(len(self.colnamesOut))  # initialize output data
            U = np.full(na, 1 / na)  # define uniform policy for later use
            B = np.unique(data[:, bIdx])
            EndPolicy = np.empty(len(B), dtype=list)  # store the end policies of each block
            row = 0
            for b in B:  # loop blocks
                ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx])  # get ns and dt of the block
                # get param for specific dt condition
                lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST
                K = (dt == 1) * (Kap - KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
                weight = rho * min(1, K / ns)  # working memory weight
                # initialize state space value matrices
                Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na), np.empty((ns, na))
                while row < nrow and int(data[row, bIdx]) == b:  # for each row of this particular block
                    corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(
                        data[row, numTrialIdx])
                    # compute softmax probabilities
                    stim_vec = np.zeros(ns)  # attention over all stimuli
                    stim_vec[stim - 1] = 1  # certain about the current stimulus
                    stim_filtered = f.softmax(stim_vec, beta_filter)  # not so much. filtered stimulus
                    Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),
                                     axis=0)  # average the Q values that do not concern the current stimulus
                    Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta), f.softmax(
                        lambdaT * Qwrong + (1 - lambdaT) * Q[stim - 1], beta)
                    initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    endPolicy[stim - 1] = Qpolicy
                    # pick action
                    try:
                        a = int(data[row, actionIdx])-1
                    except:
                        row += 1
                        continue
                    # correct action is
                    Cor = a + 1 == corAct
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1, a]), (Cor - W[stim - 1, a])
                    Q[stim - 1, a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1, a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1 / na - W)
                    # stack data
                    Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                    row += 1  # got to next row
                EndPolicy[int(b) - 1] = endPolicy
            return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filter = self.filter_beta_scale*param[self.pname.index('beta_filter')]  # filtered beta
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                stim_vec = np.zeros(ns)  # attention over all stimuli
                stim_vec[stim - 1] = 1  # certain about the current stimulus
                stim_filtered = f.softmax(stim_vec, beta_filter)  # not so much. filtered stimulus
                Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),axis=0)  # average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filter = self.filter_beta_scale*param[self.pname.index('beta_filter')]  # filtered beta
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    stim_vec = np.zeros(ns) #attention over all stimuli
                    stim_vec[stim-1] = 1 #certain about the current stimulus
                    stim_filtered = f.softmax(stim_vec, beta_filter) #not so much. filtered stimulus
                    Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),
                                     axis=0)  # average the Q values that do not concern the current stimulus
                    initPolicy = weight * f.softmax(stim_filtered.dot(W), beta) + \
                                 (1 - weight) * f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

class RLWM_WFQA2_i(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_WFQA2_i'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'beta_filter', 'lambdaST', 'lambdaDT']
        self.filter_beta_scale = 50
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filter = self.filter_beta_scale*param[self.pname.index('beta_filter')]  # filtered beta
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                stim_vec = np.zeros(ns)  # attention over all stimuli
                stim_vec[stim - 1] = 1  # certain about the current stimulus
                stim_filtered = f.softmax(stim_vec, beta_filter)  # not so much. filtered stimulus
                Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),axis=0)  # average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filter = self.filter_beta_scale*param[self.pname.index('beta_filter')]  # filtered beta
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                stim_vec = np.zeros(ns)  # attention over all stimuli
                stim_vec[stim - 1] = 1  # certain about the current stimulus
                stim_filtered = f.softmax(stim_vec, beta_filter)  # not so much. filtered stimulus
                Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),axis=0)  # average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filter = self.filter_beta_scale*param[self.pname.index('beta_filter')]  # filtered beta
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    stim_vec = np.zeros(ns) #attention over all stimuli
                    stim_vec[stim-1] = 1 #certain about the current stimulus
                    stim_filtered = f.softmax(stim_vec, beta_filter) #not so much. filtered stimulus
                    Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),
                                     axis=0)  # average the Q values that do not concern the current stimulus
                    initPolicy = weight * f.softmax(stim_filtered.dot(W), beta) + \
                                 (1 - weight) * f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

class RLWM_WFQA3(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_WFQA3'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'beta_filter', 'lambdaST', 'lambdaDT']
        self.filter_beta_scale = 50
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filter = self.filter_beta_scale*param[self.pname.index('beta_filter')]  # filtered beta
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                stim_vec = np.zeros(ns)  # attention over all stimuli
                stim_vec[stim - 1] = 1  # certain about the current stimulus
                stim_filtered = (dt==0)*stim_vec + (dt==1)*f.softmax(stim_vec, beta_filter)  # only filter during dt condition
                Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),axis=0)  # average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filter = self.filter_beta_scale*param[self.pname.index('beta_filter')]  # filtered beta
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                stim_vec = np.zeros(ns)  # attention over all stimuli
                stim_vec[stim - 1] = 1  # certain about the current stimulus
                stim_filtered = (dt==0)*stim_vec + (dt==1)*f.softmax(stim_vec, beta_filter)  # only filter during dt condition
                Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),axis=0)  # average the Q values that do not concern the current stimulus
                Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta), f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)
                initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filter = self.filter_beta_scale*param[self.pname.index('beta_filter')]  # filtered beta
        lambdaST = param[self.pname.index('lambdaST')]  # wrong credit assignment weight
        lambdaDT = param[self.pname.index('lambdaDT')]
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            lambdaT = (dt == 1) * lambdaDT + (dt == 0) * lambdaST  # wrong credit assignment weight
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    stim_vec = np.zeros(ns) #attention over all stimuli
                    stim_vec[stim-1] = 1 #certain about the current stimulus
                    stim_filtered = (dt==0)*stim_vec + (dt==1)*f.softmax(stim_vec, beta_filter)  # only filter during dt condition
                    Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),
                                     axis=0)  # average the Q values that do not concern the current stimulus
                    initPolicy = weight * f.softmax(stim_filtered.dot(W), beta) + \
                                 (1 - weight) * f.softmax(lambdaT*Qwrong + (1-lambdaT)*Q[stim - 1], beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

class RLWM_SFDA(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_SFDA'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'beta_filter', 'lambdaT']
        self.filter_beta_scale = 50
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filter = self.filter_beta_scale*param[self.pname.index('beta_filter')]  # filtered beta
        lambdaT = param[self.pname.index('lambdaT')]  # wrong credit assignment weight
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                if dt == 1:
                    stim_vec = np.zeros(ns)  # attention over all stimuli
                    stim_vec[stim - 1] = 1  # certain about the current stimulus
                    stim_filtered = (dt==0)*stim_vec + (dt==1)*f.softmax(stim_vec, beta_filter)  # only filter during dt condition
                    Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta), f.softmax(stim_filtered.dot(Q), beta)
                    initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                if dt == 0:
                    Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),axis=0)  # average the Q values that do not concern the current stimulus
                    Wpolicy, Qpolicy = f.softmax(W[stim-1], beta), f.softmax(lambdaT * Qwrong + (1 - lambdaT) * Q[stim - 1],beta)
                    initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filter = self.filter_beta_scale*param[self.pname.index('beta_filter')]  # filtered beta
        lambdaT = param[self.pname.index('lambdaT')]  # wrong credit assignment weight
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                if dt == 1:
                    stim_vec = np.zeros(ns)  # attention over all stimuli
                    stim_vec[stim - 1] = 1  # certain about the current stimulus
                    stim_filtered = (dt==0)*stim_vec + (dt==1)*f.softmax(stim_vec, beta_filter)  # only filter during dt condition
                    Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta), f.softmax(stim_filtered.dot(Q), beta)
                    initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                if dt == 0:
                    Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),axis=0)  # average the Q values that do not concern the current stimulus
                    Wpolicy, Qpolicy = f.softmax(W[stim-1], beta), f.softmax(lambdaT * Qwrong + (1 - lambdaT) * Q[stim - 1],beta)
                    initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filter = self.filter_beta_scale*param[self.pname.index('beta_filter')]  # filtered beta
        lambdaT = param[self.pname.index('lambdaT')]  # wrong credit assignment weight
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    if dt == 1:
                        stim_vec = np.zeros(ns)  # attention over all stimuli
                        stim_vec[stim - 1] = 1  # certain about the current stimulus
                        stim_filtered = (dt == 0) * stim_vec + (dt == 1) * f.softmax(stim_vec, beta_filter)  # only filter during dt condition
                        initPolicy = weight * f.softmax(stim_filtered.dot(W), beta) + \
                                     (1 - weight) * f.softmax(stim_filtered.dot(Q),beta)  # possibility distribution on action space
                    if dt == 0:
                        Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),axis=0)  # average the Q values that do not concern the current stimulus
                        initPolicy = weight * f.softmax(W[stim - 1], beta) + \
                                     (1 - weight) * f.softmax(lambdaT * Qwrong + (1 - lambdaT) * Q[stim - 1],beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - Q[stim - 1,a]), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh

class RLWM_SFDA_i(DT_RLWM_Models):  # assume for all participant
    def __init__(self):
        super().__init__()
        self.name = 'RLWM_SFDA_i'
        self.pname = ['eta', 'forget', 'rho', 'noise', 'neg_eta', 'beta_filter', 'lambdaT']
        self.filter_beta_scale = 50
        self.numParam = len(self.pname)+(len(self.Krange)>1) + (len(self.KapDiscount)>1)
    def fake_agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filter = self.filter_beta_scale*param[self.pname.index('beta_filter')]  # filtered beta
        lambdaT = param[self.pname.index('lambdaT')]  # wrong credit assignment weight
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        actionIdx = self.colnamesIn.index('action')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                if dt == 1:
                    stim_vec = np.zeros(ns)  # attention over all stimuli
                    stim_vec[stim - 1] = 1  # certain about the current stimulus
                    stim_filtered = (dt == 0) * stim_vec + (dt == 1) * f.softmax(stim_vec,
                                                                                 beta_filter)  # only filter during dt condition
                    Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta), f.softmax(stim_filtered.dot(Q), beta)
                    initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                if dt == 0:
                    Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),axis=0)  # average the Q values that do not concern the current stimulus
                    Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(
                        lambdaT * Qwrong + (1 - lambdaT) * Q[stim - 1], beta)
                    initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                try:
                    a = int(data[row, actionIdx])-1
                except:
                    row += 1
                    continue
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def agent(self, subNum, param, Kap, KapDiscount, sim=1, beta=50, na = 3):
        '''
        :param subNum: The subject number of the agent
        :param param: The corresponding parameters
        :param Kap: The corresponding capacity
        :param beta: The beta for softmax
        :param na: The number of actions
        :return: numpy data matrix of simulated data
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filter = self.filter_beta_scale*param[self.pname.index('beta_filter')]  # filtered beta
        lambdaT = param[self.pname.index('lambdaT')]  # wrong credit assignment weight
        # get coloumn index for different task variables
        bIdx = self.colnamesIn.index('block')
        nsIdx = self.colnamesIn.index('ns')
        dtIdx = self.colnamesIn.index('dt')
        corActIdx = self.colnamesIn.index('corAct')
        stimSeqIdx = self.colnamesIn.index('stimSeq')
        numTrialIdx = self.colnamesIn.index('numTrial')
        #prep work
        data = self.subject_data[subNum-1] #get the subject data
        nrow = np.size(data, 0) #get number of rows
        Data = np.ones(len(self.colnamesOut))  # initialize output data
        U = np.full(na, 1 / na) #define uniform policy for later use
        B = np.unique(data[:, bIdx])
        EndPolicy = np.empty(len(B), dtype = list) #store the end policies of each block
        row = 0
        for b in B: # loop blocks
            ns, dt = int(data[row, nsIdx]), int(data[row, dtIdx]) # get ns and dt of the block
            # get param for specific dt condition
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W, endPolicy = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na),np.empty((ns, na))
            while row<nrow and int(data[row, bIdx]) == b: #for each row of this particular block
                corAct, stim, numTrial = data[row, corActIdx], int(data[row, stimSeqIdx]), int(data[row, numTrialIdx])
                # compute softmax probabilities
                if dt == 1:
                    stim_vec = np.zeros(ns)  # attention over all stimuli
                    stim_vec[stim - 1] = 1  # certain about the current stimulus
                    stim_filtered = (dt == 0) * stim_vec + (dt == 1) * f.softmax(stim_vec,
                                                                                 beta_filter)  # only filter during dt condition
                    Wpolicy, Qpolicy = f.softmax(stim_filtered.dot(W), beta), f.softmax(stim_filtered.dot(Q), beta)
                    initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                if dt == 0:
                    Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),axis=0)  # average the Q values that do not concern the current stimulus
                    Wpolicy, Qpolicy = f.softmax(W[stim - 1], beta), f.softmax(
                        lambdaT * Qwrong + (1 - lambdaT) * Q[stim - 1], beta)
                    initPolicy = weight * Wpolicy + (1 - weight) * Qpolicy  # possibility distribution on action space
                policy = (1 - noise) * initPolicy + noise * U  # add noise
                endPolicy[stim - 1] = Qpolicy
                # pick action
                a = np.random.choice(range(na), p=policy)
                # correct action is
                Cor = a + 1 == corAct
                # state update values
                RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                # WM forgets
                W += forget * (1/na - W)
                # stack data
                Data = np.vstack((Data, np.array([subNum, b, dt, ns, numTrial, a + 1, Cor, corAct, stim, sim])))
                row += 1 #got to next row
            EndPolicy[int(b)-1] = endPolicy
        return Data[1:], EndPolicy  # remove first row
    def LLH(self, param, Data, Kap, KapDiscount, beta=50, na = 3):
        '''
        :param param: Model parameters
        :param Data: The actual data (numpy matrix
        :param Kap: Capacity parameter
        :param beta: Beta parameter
        :param na: Number of actions
        :return: The negative log likelihood (int)
        '''
        # model parameters
        eta = param[self.pname.index('eta')]  # learning rate
        forget = param[self.pname.index('forget')]  # WM forgetting
        rho = param[self.pname.index('rho')]  # initial WM weight
        noise = param[self.pname.index('noise')]
        neg_eta = param[self.pname.index('neg_eta')]  # perserverance weight to learning rate in negative feedback
        beta_filter = self.filter_beta_scale*param[self.pname.index('beta_filter')]  # filtered beta
        lambdaT = param[self.pname.index('lambdaT')]  # wrong credit assignment weight
        # get coloumn index for different task variables
        bIdx = self.colnamesOut.index('block')
        nsIdx = self.colnamesOut.index('ns')
        dtIdx = self.colnamesOut.index('dt')
        corIdx = self.colnamesOut.index('Cor')
        actionIdx = self.colnamesOut.index('action')
        stimSeqIdx = self.colnamesOut.index('stimSeq')
        #prep work
        Blocks = np.unique(Data[:, bIdx]) # list of block numbers
        U = np.full(na, 1 / na)  # define a uniform function for later use
        nrow = np.size(Data, 0) #number of rows
        llh, row = 0, 0  # initialize log likelihoods and current row
        for b in Blocks:  # loop through block
            ns, dt = int(Data[row, nsIdx]), int(Data[row, dtIdx])
            # get param for specific dt condition
            K = (dt == 1) * (Kap-KapDiscount) + (dt == 0) * Kap  # dt occupies working memory
            weight = rho * min(1, K / ns)  # working memory weight
            # initialize state space value matrices
            Q, W = np.full((ns, na), 1 / na), np.full((ns, na), 1 / na)
            while row < nrow and int(Data[row, bIdx]) == b:  # while still in this block
                stim, a, Cor = Data[row, stimSeqIdx], Data[row, actionIdx] - 1, Data[row, corIdx]  # info of the current trial
                # note, a and Cor may turn out to be NA is the subject fails to react
                if not math.isnan(Cor) and not math.isnan(a):
                    stim, a, Cor = int(stim), int(a), int(Cor)
                    # compute softmax probabilities
                    if dt == 1:
                        stim_vec = np.zeros(ns)  # attention over all stimuli
                        stim_vec[stim - 1] = 1  # certain about the current stimulus
                        stim_filtered = (dt == 0) * stim_vec + (dt == 1) * f.softmax(stim_vec, beta_filter)  # only filter during dt condition
                        initPolicy = weight * f.softmax(stim_filtered.dot(W), beta) + \
                                     (1 - weight) * f.softmax(stim_filtered.dot(Q),beta)  # possibility distribution on action space
                    if dt == 0:
                        Qwrong = np.mean(np.delete(Q, stim - 1, axis=0),axis=0)  # average the Q values that do not concern the current stimulus
                        initPolicy = weight * f.softmax(W[stim - 1], beta) + \
                                     (1 - weight) * f.softmax(lambdaT * Qwrong + (1 - lambdaT) * Q[stim - 1],beta)  # possibility distribution on action space
                    policy = (1 - noise) * initPolicy + noise * U  # add noise
                    # state update values
                    RPE_Q, RPE_W = (Cor - (weight*W[stim - 1,a] + (1-weight)*Q[stim - 1,a])), (Cor - W[stim - 1,a])
                    Q[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * eta * RPE_Q  # update Q
                    W[stim - 1,a] += (neg_eta * (Cor == 0) + (Cor == 1)) * 1 * RPE_W  # update W
                    # WM forgets
                    W += forget * (1/na - W)
                    # avoid underflow in log function
                    lh = policy[a]
                    if lh > 0:
                        llh += np.log(lh)  # update log likelihood
                row += 1  # go to the next trial
        return -llh